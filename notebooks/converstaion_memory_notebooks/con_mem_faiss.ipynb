{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio Summary Generation using watsonX Part - 2\n",
    "## 1. Importing Libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from genai.extensions.langchain import LangChainInterface\n",
    "from genai.schemas import GenerateParams\n",
    "from genai.credentials import Credentials\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Setting up the API Key and Endpoint\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GENAI_KEY\", None)\n",
    "api_url = os.getenv(\"GENAI_API\", None)\n",
    "creds = Credentials(api_key, api_endpoint=api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Setting up the model name and its parameters\n",
    "model_id = \"meta-llama/llama-2-70b-chat\"\n",
    "params = GenerateParams(\n",
    "            decoding_method=\"greedy\",\n",
    "            max_new_tokens=100,\n",
    "            min_new_tokens=10,\n",
    "            temperature=0.7\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=LangChainInterface(model=model_id, params=params, credentials=creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Function to store open the txt file, convert it in to embeddings and store it in vector Database\n",
    "def load_txt(): \n",
    "    pdf_name = 'transcriptios.txt'\n",
    "    loaders = [TextLoader(pdf_name)]\n",
    "\n",
    "    index = VectorstoreIndexCreator(\n",
    "        embedding = HuggingFaceEmbeddings(model_name='all-MiniLM-L12-v2'), \n",
    "        text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    ).from_loaders(loaders)\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Create the object of the function and call it\n",
    "index = load_txt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Deffining the PROMPT TEMPLATE\n",
    "prompt_template = \"\"\"You are a furniture persnalization assistant. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. If you not find any information about the questions answer generally by your own knowledge.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Defing the chain with parameters\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=index.vectorstore.as_retriever(), chain_type_kwargs=chain_type_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Test time\n",
    "query = \"Do you prefer second-hand or new furniture??\"\n",
    "\n",
    "print(qa.run(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
