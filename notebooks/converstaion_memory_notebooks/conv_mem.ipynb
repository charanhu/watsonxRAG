{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from genai.extensions.langchain import LangChainInterface\n",
    "from genai.schemas import GenerateParams\n",
    "from genai.credentials import Credentials\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key and URL from the environment variables\n",
    "api_key = os.getenv(\"GENAI_KEY\", None)\n",
    "api_url = os.getenv(\"GENAI_API\", None)\n",
    "\n",
    "# Create a Credentials object to pass to the LangChainInterface\n",
    "creds = Credentials(api_key, api_endpoint=api_url)\n",
    "\n",
    "# Create a GenerateParams object to pass to the LangChainInterface\n",
    "params = GenerateParams(\n",
    "            decoding_method=\"greedy\",\n",
    "            max_new_tokens=1000,\n",
    "            min_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "# Create a LangChainInterface object to use for generating text\n",
    "llm = LangChainInterface(model=\"meta-llama/llama-2-70b-chat\", params=params, credentials=creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TEMPLATE = \"\"\"Tha following exchange is a friendly conversation between a human and AI. Tha AI is talktive and provides a lot of specific details from its context.\n",
    "If the AI does not know the answer to the question, it will truthfully say it does not know.\n",
    "\n",
    "Relavant pieces of previous conversation:\n",
    "{history}\n",
    "\n",
    "Current conversation:\n",
    "Humans: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=DEFAULT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_chain = ConversationChain(llm, prompt=PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
