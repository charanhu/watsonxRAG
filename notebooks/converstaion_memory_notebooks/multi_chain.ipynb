{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from genai.extensions.langchain import LangChainInterface\n",
    "from genai.schemas import GenerateParams\n",
    "from genai.credentials import Credentials\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "load_dotenv()\n",
    "# Get the API key and URL from the environment variables\n",
    "api_key = os.getenv(\"GENAI_KEY\", None)\n",
    "api_url = os.getenv(\"GENAI_API\", None)\n",
    "# Create a Credentials object to pass to the LangChainInterface\n",
    "creds = Credentials(api_key, api_endpoint=api_url)\n",
    "# Create a GenerateParams object to pass to the LangChainInterface\n",
    "params = GenerateParams(\n",
    "            decoding_method=\"greedy\",\n",
    "            max_new_tokens=500,\n",
    "            min_new_tokens=50,\n",
    "            temperature=0.7,\n",
    "            stop_sequences=[\"Friend:\"],\n",
    "        )\n",
    "# Create a LangChainInterface object to use for generating text\n",
    "llm = LangChainInterface(model=\"meta-llama/llama-2-70b-chat\", params=params, credentials=creds)\n",
    "\n",
    "# Now we can override it and set it to \"Friend\"\n",
    "template = \"\"\"You will be helping for funrniture company by responding customer questions. Like personal preferences for the customers. The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. If you not find any information about the questions answer generally by your own knowledge.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Friend: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "#template=template.format(context=context, history=\"\", input=\"\")\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "conversation = ConversationChain(\n",
    "    prompt=PROMPT,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory(human_prefix=\"Friend\"),\n",
    ")\n",
    "# query = \"Do you prefer second-hand or new furniture??\"\n",
    "# conversation.predict(input=query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio Summary Generation using watsonX Part - 2\n",
    "## 1. Importing Libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from genai.extensions.langchain import LangChainInterface\n",
    "from genai.schemas import GenerateParams\n",
    "from genai.credentials import Credentials\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "## 4. Function to store open the txt file, convert it in to embeddings and store it in vector Database\n",
    "def load_txt(): \n",
    "    pdf_name = 'transcriptios.txt'\n",
    "    loaders = [TextLoader(pdf_name)]\n",
    "\n",
    "    index = VectorstoreIndexCreator(\n",
    "        embedding = HuggingFaceEmbeddings(model_name='all-MiniLM-L12-v2'), \n",
    "        text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    ).from_loaders(loaders)\n",
    "\n",
    "    return index\n",
    "## 5. Create the object of the function and call it\n",
    "index = load_txt()\n",
    "## 6. Deffining the PROMPT TEMPLATE\n",
    "prompt_template = \"\"\"You are a furniture persnalization assistant. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. If you not find any information about the questions answer generally by your own knowledge.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "## 7. Defing the chain with parameters\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=index.vectorstore.as_retriever(), chain_type_kwargs=chain_type_kwargs)\n",
    "\n",
    "conversation_chain = ConversationChain(\n",
    "    prompt=PROMPT,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory(human_prefix=\"Friend\"),\n",
    ")\n",
    "\n",
    "\n",
    "# ## 8. Test time\n",
    "# query = \"Do you prefer second-hand or new furniture??\"\n",
    "\n",
    "# print(qa.run(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the overall chain where we run these two chains in sequence.\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "overall_chain = SimpleSequentialChain(\n",
    "    chains=[qa, conversation], verbose=True\n",
    ")\n",
    "\n",
    "review = overall_chain.run(\"Do you prefer second-hand or new furniture?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
